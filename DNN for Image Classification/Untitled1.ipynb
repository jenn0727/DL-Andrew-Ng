{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z) \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layers_dims[l-1])* 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    cache = []\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b' + str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1./m * np.sum(Y*np.log(AL)+(1-Y)* np.log(1-AL)) \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    cache_L = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, cache_L, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        cache_l = caches[l]\n",
    "        grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] = linear_activation_backward(grads[\"dA\" + str(l + 2)], cache_l, \"relu\")\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate * grads[\"dW\" + str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate * grads[\"db\" + str(l+1)]  \n",
    "    return parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#L-layer model\n",
    "\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = [] \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for i in range (0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x93a6908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "m_train = train_x_orig.shape[0] #209\n",
    "num_px = train_x_orig.shape[1]  #64\n",
    "m_test = test_x_orig.shape[0]   #50\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T  \n",
    "# The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "#12288=64*64*3\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255. \n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
